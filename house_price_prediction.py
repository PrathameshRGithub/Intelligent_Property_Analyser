# -*- coding: utf-8 -*-
"""House_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q0L-NtpzlpJRJWxKgR-pJf1cTGA7Wy1g

Data Collection
"""

# Reading necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Load the train dataset
train = pd.read_csv('/content/drive/MyDrive/TCS RIO/train.csv')

# load the test dataset
test = pd.read_csv('/content/drive/MyDrive/TCS RIO/test.csv')

# Check number of rows and columns
train.shape

# Expanding the display output
pd.set_option('display.max_columns',1000)
pd.set_option('display.max_rows',1000)

# Displaying first 5 rows
train.head()

# check the shape for test dataset
test.shape

# Display sample data for test dataset
test.head(5)

# merge train and test dataset
data =  pd.concat([train,test])
data.shape

# Display merged data
data.head()

# Observe columns datatypes and null values
data.info()

# setting ID column as index
data = data.set_index('Id')

"""# Exploratory Data Analysis
EDA Report

Uncomment the below comment before executing the run to get the html EDA report
"""

# Installing sweetviz for Exploratory Data Analysis(EDA) Report
#!pip install sweetviz

# Create complete report for EDA
#import sweetviz
#my_report  = sweetviz.analyze(data)
#my_report.show_html('EDAReport.html')

# Summarize the data
data.describe()

# Observe total numerical variables in data.
Numerical_cols=data.select_dtypes(include=['int','float']).columns
print(Numerical_cols)
print('Total Number of Numerical columns:',len(Numerical_cols))

# Observe categorical variables in the data
Categorical_cols=data.select_dtypes(include=['object']).columns
print(Categorical_cols)
print('Total Number of Categorical columns:',len(Categorical_cols))

"""# Handling Missing Values"""

# Check the null values in the data
null_percentage = data.isnull().sum()/data.shape[0]*100
null_percentage

# Check the percentage of null values in columns sorted in descending order
null_percentage.sort_values(ascending=False)

# Total columns that have null values
total_missing_val_col = null_percentage[null_percentage > 0]
print('Total attributes with missing values:',len(total_missing_val_col))

# columns with more than 20 % null values
print(null_percentage[null_percentage>20])

# remaining columns with less than 20% null values
print(total_missing_val_col[total_missing_val_col<20].sort_values(ascending=False))

# categorical columns with null values
Categorical_cols_with_missval = total_missing_val_col[total_missing_val_col.keys().isin(Categorical_cols)]
print("Total number of categorical features with missing values:",len(Categorical_cols_with_missval))

# Numerical columns with null values
Numerical_cols_with_missval =total_missing_val_col[total_missing_val_col.keys().isin(Numerical_cols)]
print("Total number of Numerical Features with missing values:",len(Numerical_cols_with_missval))

# all columns in the dataset
data.columns

# Drop the columns with missing values more than 20 percent
data  = data.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1)

# check shape of data after dropping
data.shape

data.columns

data.head()

data.tail()

# number unique values in categorical columns
for i in Categorical_cols:
  if i in data.columns:
    print(i + '\t' + str(len(data[i].unique())))

# Unique values in categorical columns
for i in Categorical_cols:
  if i in data.columns:
    print("Unique values of: {} ({})\n{}\n".format(i,len(data[i].unique()),data[i].unique()))

# null values in categorical columns (less than 20%)
Categorical_cols_with_missval=Categorical_cols_with_missval[Categorical_cols_with_missval<20].sort_values(ascending=False)
Categorical_cols_with_missval

# we have 4 missing values
data['MSZoning'].isnull().sum()

data['MSZoning'].value_counts()# we have 4 missing values

# Visualize the count of unique values in column MSZoning with the help of countplot
sns.countplot(x='MSZoning',data=data)

# filling the null values with Mode value
data['MSZoning'].fillna(data['MSZoning'].mode()[0],inplace=True)

# see the result after handling missing value
data['MSZoning'].isnull().sum()

# we have 2 missing values in Utilities values
data['Utilities'].isnull().sum()

# value counts of unique values in Utilities
data['Utilities'].value_counts()

# countplot
sns.countplot(x='Utilities',data=data)

# replacing null values with Mode
data['Utilities'].replace(np.nan,data['Utilities'].mode()[0],inplace=True)

# check for null value after replacing
data['Utilities'].isnull().sum()

# null valu in GarageCond column
data['GarageCond'].isnull().sum()

sns.countplot(x='GarageCond',data=data)

# Replacing with mode
data['GarageCond'].fillna(data['GarageCond'].mode()[0],inplace=True)

data['GarageCond'].isnull().sum()

# remove the update columns from the list
Updated_Categorical_cols_with_missval=Categorical_cols_with_missval.drop(index=['GarageCond','Utilities','MSZoning'])

# Remaining categorical columns with null values
Updated_Categorical_cols_with_missval

# check for total number of null values in each remaining each categorical column
for i in Updated_Categorical_cols_with_missval.index:
  print('Missing Values in',i,':',data[i].isnull().sum())

# Create a list of Mode values for categorical columns
modes=[]
for i in Updated_Categorical_cols_with_missval.index:
  print("Mode Value of",i,':',data[i].mode()[0])
  modes.append(data[i].mode()[0])

# Replace the null values with Mode values of each column for each column respectively
for (i,j) in zip(Updated_Categorical_cols_with_missval.index,modes):
     data[i].fillna(j,inplace=True)

# check after replacing
for i in Updated_Categorical_cols_with_missval.index:
  print('Missing Values in',i,':',data[i].isnull().sum())

# Numerical columns
Numerical_cols_with_missval.sort_values(ascending=False)

# visualizing numerical data using boxplot
sns.boxplot(x='LotFrontage',data=data)
print('Median:',data['LotFrontage'].median(),'Mean:',data['LotFrontage'].mean())

sns.boxplot(x='GarageYrBlt',data=data)
print('Median:',data['GarageYrBlt'].median(),'Mean:',data['GarageYrBlt'].mean())

#drop the target variable
Numerical_cols_with_missval=Numerical_cols_with_missval.drop(index='SalePrice')

data['MasVnrArea']

# Display number of null values in Numerical columns
for i in Numerical_cols_with_missval.index:
  print('Missing Values in',i,':',data[i].isnull().sum())

# create a list for mean values in
mean_values=[]
for i in Numerical_cols_with_missval.index:
  print("Mean Value of",i,':',np.round(data[i].mean()))
  mean_values.append(np.round(data[i].mean()))

# filling null values with mean values
for (i,j) in zip(Numerical_cols_with_missval.index,mean_values):
     data[i].fillna(j,inplace=True)

# check for any null values after replacing
for i in Numerical_cols_with_missval.index:
  print('Missing Values in',i,':',data[i].isnull().sum())



# check if any null values in complete dataset
data.isnull().any()

# Summary of target variable
data['SalePrice'].describe()

#  Distribution plot for target variable
dist = sns.distplot(data['SalePrice'])
dist.legend(["Skewness: {:.2f}".format(data['SalePrice'].skew())])

"""After plotting for SalePrice we can say that the data is skewed"""

# plot heatmap for correlation
correlation = data.corr()
plt.figure(figsize=(30,25))
sns.heatmap(correlation,annot=True,cmap='Spectral')



# selecting columns with high correlation with SalePrice
highly_correlated = correlation.index[abs(correlation['SalePrice'])>=0.5]
highly_correlated

# heatmap of highly_correlated columns
sns.heatmap(data[highly_correlated].corr(),annot=True,cmap='Spectral_r')

# regression plot for highly_correlated columns
plt.figure(figsize=(18,14))
for i in range(len(highly_correlated)):
  if i <= 9:
    plt.subplot(4,4,i+1)
    plt.subplots_adjust(hspace=0.8,wspace=0.8)
    sns.regplot(data=data,x=highly_correlated[i],y='SalePrice')

data[data['BsmtQual']=='Gd']['BsmtExposure'].mode()[0]

data['BsmtQual'].unique()

# Preprocessing numerical column
Num = data[Numerical_cols]
Num

# check for null values if any
Num.isnull().sum()

# Observe unique values in numerical column
# here we can see that we have huge data with 0 value
data['MasVnrArea'].value_counts().head()

# visualizing distribution plot for MasVnrArea column
sns.histplot(data=data,x='MasVnrArea')

data['MasVnrArea']

np.seterr(divide = 'ignore')

# Performing log transformation on the column data to visualize the output
print(data['MasVnrArea'].skew())
mas = np.log(data['MasVnrArea'])
sns.histplot(data=mas)

# Performing Square root transformation and visualize the output
mas1 = np.sqrt(data['MasVnrArea'])
sns.histplot(data=mas1)

# Performing cuberoot transformation on the data and visualize
mas2 = np.cbrt(data['MasVnrArea'])
sns.histplot(data=mas2)

data.info()

(data['MasVnrArea']==0.0).any()

(data['BsmtFinSF1']==0).any()

# Create a list of columns names with 0 values
col_with_zero_val=[]
for i in data:
  if (data[i]==0.0).any()==True or (data[i]==0).any()==True:
    col_with_zero_val.append(i)
col_with_zero_val

# rows with 0 values in BsmtFinSF1 column
(data['BsmtFinSF1']==0).sum()

# Total number of rows with zero values
for i in col_with_zero_val:
  print(i,':',(data[i]==0.0).sum())

# Some columns that are categorical but have int or float datatype
convert_into_cat = ['MSSubClass','YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']
for i in convert_into_cat:
  data[i]= data[i].astype(str)

# check column datatype for column
for i in convert_into_cat:
  print(i," -datatype :",data[i].dtypes,)

# Mosold column have month names so convert it using a function
import calendar
data['MoSold']=data['MoSold'].apply(lambda x : calendar.month_abbr[x])

# see the result
data['MoSold'].unique()

# check total categorical column after updation
Categorical_cols=data.select_dtypes(include=['object']).columns
print(Categorical_cols)
print('Total Number of Categorical columns:',len(Categorical_cols))

# unique value in cateogorical columns
for i in Categorical_cols:
  if i in data.columns:
    print("Unique values of: {} ({})\n{}\n".format(i,len(data[i].unique()),data[i].unique()))

"""# Encoding"""

# Ordinal encoding of some categorical columns
from pandas.api.types import CategoricalDtype
data['ExterQual']= data['ExterQual'].astype(CategoricalDtype(categories=['Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes

data['ExterQual'].unique()

data['BsmtQual'].unique()

data['BsmtQual']= data['BsmtQual'].astype(CategoricalDtype(categories=['Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes
data['BsmtCond']= data['BsmtCond'].astype(CategoricalDtype(categories=['Po','Fa', 'TA', 'Gd',], ordered = True)).cat.codes
data['BsmtExposure']= data['BsmtExposure'].astype(CategoricalDtype(categories=['No', 'Mn', 'Gd', 'Av'], ordered = True)).cat.codes
data['BsmtFinType1']= data['BsmtFinType1'].astype(CategoricalDtype(categories=['Unf','LwQ','Rec','BLQ','ALQ','GLQ'], ordered = True)).cat.codes
data['BsmtFinType2']= data['BsmtFinType2'].astype(CategoricalDtype(categories=['Unf','LwQ','Rec','BLQ','ALQ','GLQ'], ordered = True)).cat.codes
data['HeatingQC']= data['HeatingQC'].astype(CategoricalDtype(categories=['Po','Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes
data['Electrical']= data['Electrical'].astype(CategoricalDtype(categories=['Mix','FuseP','FuseF','FuseA','SBrkr'], ordered = True)).cat.codes
data['KitchenQual']= data['KitchenQual'].astype(CategoricalDtype(categories=['Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes
data['Functional']= data['Functional'].astype(CategoricalDtype(categories=['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], ordered = True)).cat.codes
data['GarageFinish']= data['GarageFinish'].astype(CategoricalDtype(categories=['Unf','RFn','Fin'], ordered = True)).cat.codes
data['GarageQual']= data['GarageQual'].astype(CategoricalDtype(categories=['Po','Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes
data['GarageCond']= data['GarageCond'].astype(CategoricalDtype(categories=['Po','Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes
data['PavedDrive']= data['PavedDrive'].astype(CategoricalDtype(categories=['N','P','Y'], ordered = True)).cat.codes
data['Utilities']= data['Utilities'].astype(CategoricalDtype(categories=['NoSeWa','AllPub'], ordered = True)).cat.codes
data['ExterCond']= data['ExterCond'].astype(CategoricalDtype(categories=['Po','Fa', 'TA', 'Gd', 'Ex'], ordered = True)).cat.codes

# check the result after encoding
data['Utilities'].value_counts()

data['Electrical'].unique()

#Total column after dropping some columns
print(len(data.columns))
print(data.columns)

data.head()

# check the datatype
data['MSSubClass'].dtypes

# saving file after part pre processing
data.to_csv('file_for_featureeng.csv')

data.info()

print(Numerical_cols)
print(len(Numerical_cols))

Numerical_cols=Numerical_cols.drop(['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','GarageYrBlt','YrSold','MoSold','SalePrice'])

print(Numerical_cols)# created columns for plotting distribution plot and transforming continuous valued variables
print(len(Numerical_cols))

# Visualize the distribution plot for skewed columns
plt.figure(figsize=(20,20))
for i in range(len(Numerical_cols)):
  if i <= 28:
    plt.subplot(7,4,i+1)
    plt.subplots_adjust(hspace = 0.5,wspace=0.5)
    plot=sns.distplot(data[Numerical_cols[i]])
    plot.legend(['Skewness:{:.2f}'.format(data[Numerical_cols[i]].skew())])

# Log transformation of numerical columns to treat 0 values in the columns.
for i in Numerical_cols:
  data[i] = np.log(data[i]+1)

# save file after transformation
dataframe = data
dataframe.to_csv('data_after_removing_skewness.csv')

# Visualization after transforming
plt.figure(figsize=(20,20))
for i in range(len(Numerical_cols)):
  if i <= 28:
    plt.subplot(7,4,i+1)
    plt.subplots_adjust(hspace = 0.5,wspace=0.5)
    plot=sns.distplot(data[Numerical_cols[i]])
    plot.legend(['Skewness:{:.2f}'.format(data[Numerical_cols[i]].skew())])

# Log Transformation of SalePrice target column
SalePrice=np.log(train['SalePrice']+1)

# display
SalePrice

data.head()

# check correlation after transformation
plt.figure(figsize=(40,25))
sns.heatmap(dataframe.corr(),annot=True,cmap = 'Spectral')

# final categorical columns
final_cat_col=data.select_dtypes(include=['object']).columns
print(final_cat_col)
print(len(data.select_dtypes(include=['object']).columns))

# highly correlated columns
correlation_final=dataframe.corr()
Final_highly_correlated = correlation_final.index[abs(correlation_final['SalePrice'])>=0.5]
Final_highly_correlated

# heatmap
sns.heatmap(data[Final_highly_correlated].corr(),annot=True,cmap='Spectral_r')

dataframe

# drop target column
Final_highly_correlated = Final_highly_correlated.drop('SalePrice')

Final_highly_correlated

data.head()

# saving file after transforming
SalePrice.to_csv('SalePrice.csv')