# -*- coding: utf-8 -*-
"""Final_model_selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gwn30PdJ35_un-Hr3q2V2ogotCcmBmLk
"""

# necessary libraries
import pandas as pd
import numpy as np

# load dataset which cleaned
dataset = pd.read_csv('/content/drive/MyDrive/TCS RIO/file_for_featureeng.csv')
train = pd.read_csv('/content/drive/MyDrive/TCS RIO/train.csv')
test = pd.read_csv('/content/drive/MyDrive/TCS RIO/test.csv')
unskewd_data = pd.read_csv('/content/drive/MyDrive/TCS RIO/data_after_removing_skewness.csv')

# sample rows
dataset.head()

dataset = dataset.set_index('Id')

# check data info
dataset.info()

# necessary processing
convert_into_cat = ['MSSubClass','YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']
for i in convert_into_cat:
  dataset[i]= dataset[i].astype(str)

# total categorical variables
cat_feat = list(dataset.loc[:,dataset.dtypes == 'object'].columns.values)
len(cat_feat)

dataset.info()

cat_feat

# using get_dummies to for one hot encoding
dummy_drop = []
clean_data = dataset
for i in cat_feat:
    dummy_drop += [i + '_' + str(dataset[i].unique()[-1])]

data = pd.get_dummies(dataset, columns = cat_feat)
data = data.drop(dummy_drop, axis = 1)

data.shape

# drop sale price column
data=data.drop('SalePrice',axis=1)

# Scaling data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
data_scaled = sc.fit_transform(data)

# Split the data
train_len=len(train)
x_train = data_scaled[:train_len]
x_test = data_scaled[train_len:]
y_train = train['SalePrice'][:train_len]

# check columns and rows in data
x_test.shape

# Target variable
y_train

# Build Linear Regression algorithm and fit on data
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train,y_train)

# predict and evaluate
from sklearn.metrics import r2_score
y_train_pred = lr.predict(x_train)
print(r2_score(y_train,y_train_pred))

pd.set_option('display.max_rows', 1000)
pd.set_option('display.max_columns', 1000)

# Build Support Vector Machine algorithm fit,predict and evaluate with hyperparmater tuning
from sklearn.svm import SVR
kernel = ['linear','rbf','poly']
for i in kernel:
   SVM = SVR(kernel=i)
   SVM.fit(x_train,y_train)
   svm_pred = SVM.predict(x_train)
   r2=r2_score(svm_pred,y_train)
   print('r2_score is :',r2,'for',i,'kernel')

# Try another ML algorithm Decision Tree
from sklearn.tree import DecisionTreeRegressor
for i in range(1,21):
  dt = DecisionTreeRegressor(criterion='squared_error',splitter='best',max_depth=i,random_state=60)
  dt.fit(x_train,y_train)
  dt_pred = dt.predict(x_train)
  r2_dt = r2_score(dt_pred,y_train)
  print('r2_score is',r2_dt,'with max_depth of:',i)

# Another Algorithm - Gradient Boosting Regressor
from sklearn.ensemble import GradientBoostingRegressor
for i in range(1,11):
    GBR = GradientBoostingRegressor(learning_rate=0.2,max_depth=i)
    GBR.fit(x_train,y_train)
    gbr_pred=GBR.predict(x_train)
    r2_gbr = r2_score(gbr_pred,y_train)
    print('r2_score is:',r2_gbr,'for',i,'depth')

# Select the model with best Hyperparameter
GBR = GradientBoostingRegressor(learning_rate=0.2,max_depth=9)
GBR.fit(x_train,y_train)
gbr_pred=GBR.predict(x_train)

gbr_pred

r2_score(gbr_pred,y_train)

y_pred_gbr=GBR.predict(x_test)

y_pred_gbr

# Feature Selection technique - Recursive Feature Elimination
from sklearn.feature_selection import RFE
rfe=RFE(lr)
rfe=rfe.fit(x_train,y_train)

# Important features
list(zip(data.columns,rfe.ranking_))

y_pred_rfe=rfe.predict(x_train)

r2_score(y_pred_rfe,y_train)

# Random Forest algorithm
from sklearn.ensemble import RandomForestRegressor
rf= RandomForestRegressor(n_estimators=100,criterion='squared_error',random_state=80,max_depth=18,max_features=20)
rf.fit(x_train,y_train)
rf_pred=rf.predict(x_train)

# Evaluate the prediction
r2_score(rf_pred,y_train)

# Hyperparameter tuning for Random forest algorithm
for i in range(1,20):
  rfc= RandomForestRegressor(n_estimators=100,criterion='squared_error',random_state=80,max_depth=i)
  rfc.fit(x_train,y_train)
  rfc_pred=rfc.predict(x_train)
  score=r2_score(rfc_pred,y_train)
  print(i ,':',score)

# Get important features used for prediction
importances = rf.feature_importances_

# Sort the feature importance in descending order

sorted_indices = np.argsort(importances)[::-1]

feat_labels = data.columns

for i in range(x_train.shape[1]):
    print("%2d) %-*s %f" % (i + 1, 20, feat_labels[sorted_indices[i]],importances[sorted_indices[i]]))

""" We  have selected first 20 important features and created a Final data for model building and prediction."""

# we selected first 20 important features and created a Final data for buidlin
datanew = data[['GrLivArea', 'OverallQual' ,'KitchenQual','GarageArea','GarageCars','TotalBsmtSF','ExterQual' ,'1stFlrSF' ,'BsmtQual','GarageFinish' ,'FullBath', 'TotRmsAbvGrd', 'Foundation_PConc','2ndFlrSF','BsmtFinSF1' ,'Fireplaces' ,'LotArea' ,'LotFrontage','MasVnrArea' ,'BsmtFinType1']]
datanew

datanew.to_csv('housedata.csv')

datanew.info()

data['Fireplaces'].unique()

"""We will test the model first on the data(with selected features) to check  how it is performing and tune & optimize if needed later then Build the final model"""

# Scaling the data
data_scaled_new = sc.fit_transform(datanew)
x_train_new=data_scaled_new[:train_len]

# fit the model on data
rf.fit(x_train_new,y_train)

# predict the output for train dataset
x_pred_new = rf.predict(x_train_new)

r2_score(x_pred_new,y_train)

x_test_new=data_scaled_new[train_len:]

rf.predict(x_test_new)

# Final Model
RF = RandomForestRegressor(n_estimators=100,criterion='squared_error',random_state=80,max_depth=18)

# Spilt the data with important features
X_train = datanew[:train_len]
X_test = datanew[train_len:]

# Create a pipeline for Scaling and fitting model
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

pipeline = Pipeline([('Scaler', StandardScaler(with_mean = False)),  # Applying StandardScaler to all columns
    ('Model',RF)
])

# fit pipeline
pipeline.fit(X_train,y_train)

#Evaluation how model is performing
pipeline.score(X_train,y_train)

r2_score(pipeline.predict(X_train),y_train)

np.round(pipeline.predict(X_test))

# Final Prediction for test data for submission
y_pred=np.round(pipeline.predict(X_test))
submit_test = pd.concat([test['Id'],pd.DataFrame(y_pred)], axis=1)
submit_test.columns=['Id', 'SalePrice']
submit_test

submit_test.to_csv('Predicted_for_test.csv')

# create pkl file for flask deployment
import pickle
Reg_model = '/content/drive/MyDrive/TCS RIO/Regmodel.pkl'
reg_pkl = open(Reg_model,'wb')
pickle.dump(pipeline,reg_pkl)
reg_pkl.close()

# check the working of model
Readmodel = open(Reg_model,'rb') # r-read,b-binary
tmp=pickle.load(Readmodel)
tmp

tmp.predict(X_test)

X_test.info()

# Evaluation of the model
def evaluate(model,x_test,y_test):
    predictions = model.predict(x_test)
    errors = abs(predictions -y_test)
    mape = 100 * np.mean(errors /y_test)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))
    return accuracy

predictions = pipeline.predict(X_train)

predictions

errors = abs(predictions -y_train.values)
errors

mape = 100 * np.mean(errors /y_train.values)
mape

np.mean(errors)

accuracy = evaluate(pipeline,X_train,y_train)